{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on the divergence of RoBERTa embeddings\n",
    "\n",
    "The purpose of this notebook is to answer this question:\n",
    "*Does the cross-entopy of the K-means clusterization of the output embeddings of the RoBERTa model with the true labels of the dataset increase when we apply homoglyph-based adversarial attacks to the input text?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SJTU-CL/RoBERTa-large-ArguGPT-sent\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"SJTU-CL/RoBERTa-large-ArguGPT-sent\")\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information about the model architecture\n",
    "print(model)\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the classification head\n",
    "model_without_head = model.roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset. For this example, we'll use the CHEAT dataset.\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"silverspeak/cheat\")['train']\n",
    "\n",
    "real_labels = dataset['generated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the output embeddings of the model for the dataset\n",
    "def get_mean_embedding(text, first_token = False):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {name: tensor.to(model.device) for name, tensor in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model_without_head(**inputs)\n",
    "    if first_token:\n",
    "        # Take the first token of the output embeddings\n",
    "        return outputs.last_hidden_state.cpu()[:, 0, :]\n",
    "    else:\n",
    "        # Mean pooling of the output embeddings, otherwise we would have to use the [CLS] token\n",
    "        return outputs.last_hidden_state.cpu().mean(dim=1)\n",
    "\n",
    "embeddings = []\n",
    "for i in range(len(dataset)):\n",
    "    text = dataset[i]['text']\n",
    "    embeddings.append(get_mean_embedding(text))\n",
    "# If the embeddings have 2 dimensions, we need to stack them\n",
    "if len(embeddings[0].shape) == 2:\n",
    "    embeddings = torch.stack(embeddings)\n",
    "# If the embeddings have 3 dimensions, we need to concatenate them\n",
    "elif len(embeddings[0].shape) == 3:\n",
    "    embeddings = torch.cat(embeddings, dim=1)\n",
    "\n",
    "# squeeze the embeddings\n",
    "embeddings = embeddings.squeeze()\n",
    "print(f'Shape of the embeddings: {embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, apply the rewriting algorithm to the embeddings with 10% replacement rate\n",
    "from silverspeak.homoglyphs.random_attack import random_attack\n",
    "\n",
    "attacked_embeddings = []\n",
    "for i in range(len(dataset)):\n",
    "    text = dataset[i]['text']\n",
    "    attacked_text = random_attack(text, percentage=0.1)\n",
    "    attacked_embeddings.append(get_mean_embedding(attacked_text))\n",
    "# If the embeddings have 2 dimensions, we need to stack them\n",
    "if len(attacked_embeddings[0].shape) == 2:\n",
    "    attacked_embeddings = torch.stack(attacked_embeddings)\n",
    "# If the embeddings have 3 dimensions, we need to concatenate them\n",
    "elif len(attacked_embeddings[0].shape) == 3:\n",
    "    attacked_embeddings = torch.cat(attacked_embeddings, dim=1)\n",
    "\n",
    "# squeeze the embeddings\n",
    "attacked_embeddings = attacked_embeddings.squeeze()\n",
    "print(f'Shape of the attacked embeddings: {attacked_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Projections\n",
    "\n",
    "Let's see where every text is projected in the embedding space. We will use the dimensionality reduction techniques (UMAP, PCA, t-SNE) to visualize the embeddings of the RoBERTa model before and after the adversarial attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Save the figures in 'figures/'\n",
    "# If __file__ is not defined, define it (this may happen in interactive environments)\n",
    "if \"__file__\" not in globals():\n",
    "    __file__ = Path(\"visualization.py\").resolve()\n",
    "# Make sure that the output directory exists\n",
    "Path(__file__).parent.parent.joinpath(\"figures\").mkdir(exist_ok=True)\n",
    "figures_dir = Path(__file__).parent.parent / \"figures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_embeddings(embeddings, real_labels, file_name):\n",
    "    human_embeddings = embeddings[np.array(real_labels) == False]\n",
    "    machine_embeddings = embeddings[np.array(real_labels) == True]\n",
    "    plt.scatter(machine_embeddings[:, 0], machine_embeddings[:, 1], c='red', label='AI', alpha=0.5, s=8)\n",
    "    plt.scatter(human_embeddings[:, 0], human_embeddings[:, 1], c='blue', label='Human', alpha=0.5, s=8)\n",
    "    # Show a legend for the real labels\n",
    "    # plt.title('Original embeddings')#, fontname='Times New Roman')\n",
    "    plt.legend()\n",
    "    # Save as PDF\n",
    "    plt.savefig(figures_dir / file_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do an UMAP visualization of the embeddings\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "umap = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42, n_components=2, local_connectivity=5)\n",
    "reduced_umap_embeddings = umap.fit_transform(embeddings.numpy())\n",
    "plot_embeddings(reduced_umap_embeddings, real_labels, file_name=figures_dir / 'divergence_embeddings_umap_original.pdf')\n",
    "\n",
    "# Now, do the same for the attacked embeddings\n",
    "reduced_attacked_embeddings = umap.fit_transform(attacked_embeddings.numpy())\n",
    "plot_embeddings(reduced_attacked_embeddings, real_labels, file_name=figures_dir / 'divergence_embeddings_umap_attacked.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, do the same for PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced_pca_embeddings = pca.fit_transform(embeddings.numpy())\n",
    "plot_embeddings(reduced_pca_embeddings, real_labels, file_name=figures_dir / 'divergence_embeddings_pca_original.pdf')\n",
    "\n",
    "# Now, do the same for the attacked embeddings\n",
    "reduced_attacked_embeddings = pca.fit_transform(attacked_embeddings.numpy())\n",
    "plot_embeddings(reduced_attacked_embeddings, real_labels, file_name=figures_dir / 'divergence_embeddings_pca_attacked.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, do the same for t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "reduced_tsne_embeddings = tsne.fit_transform(embeddings.numpy())\n",
    "plot_embeddings(reduced_tsne_embeddings, real_labels, file_name=figures_dir / 'divergence_embeddings_tsne_original.pdf')\n",
    "\n",
    "# Now, do the same for the attacked embeddings\n",
    "reduced_attacked_embeddings = tsne.fit_transform(attacked_embeddings.numpy())\n",
    "plot_embeddings(reduced_attacked_embeddings, real_labels, file_name=figures_dir / 'divergence_embeddings_tsne_attacked.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other experiment: what happens if we combine both the original and the attacked embeddings and observe where they are in the embedding space?\n",
    "combined_embeddings = torch.cat([embeddings, attacked_embeddings])\n",
    "reduced_combined_embeddings = umap.fit_transform(combined_embeddings.numpy())\n",
    "reduced_original_embeddings = reduced_combined_embeddings[:len(embeddings)]\n",
    "reduced_attacked_embeddings = reduced_combined_embeddings[len(embeddings):]\n",
    "human_embeddings_original = reduced_original_embeddings[np.array(real_labels) == False]\n",
    "machine_embeddings_original = reduced_original_embeddings[np.array(real_labels) == True]\n",
    "human_embeddings_attacked = reduced_attacked_embeddings[np.array(real_labels) == False]\n",
    "machine_embeddings_attacked = reduced_attacked_embeddings[np.array(real_labels) == True]\n",
    "plt.scatter(machine_embeddings_original[:, 0], machine_embeddings_original[:, 1], c='red', label='AI (original)', alpha=0.5, s=8)\n",
    "plt.scatter(human_embeddings_original[:, 0], human_embeddings_original[:, 1], c='blue', label='Human (original)', alpha=0.5, s=8)\n",
    "plt.scatter(machine_embeddings_attacked[:, 0], machine_embeddings_attacked[:, 1], c='green', label='AI (attacked)', alpha=0.5, s=8, marker='x')\n",
    "plt.scatter(human_embeddings_attacked[:, 0], human_embeddings_attacked[:, 1], c='purple', label='Human (attacked)', alpha=0.5, s=8, marker='x')\n",
    "# Show a legend for the real labels\n",
    "# plt.title('Original embeddings')#, fontname='Times New Roman')\n",
    "plt.legend()\n",
    "# Save as PDF\n",
    "plt.savefig(figures_dir / 'divergence_embeddings_umap_combined.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "silverspeak-tests",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
